{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSxnCMd-Vlej"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('./data/UCI_Credit_Card.csv')\n",
        "\n",
        "# Preprocessing\n",
        "data.rename(columns=lambda x: x.strip().lower().replace(' ', '_'), inplace=True)\n",
        "data['default'] = data['default_payment_next_month']\n",
        "\n",
        "# Feature Engineering: Create new features\n",
        "data['credit_utilization'] = data[['bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6']].mean(axis=1) / data['limit_bal']\n",
        "data['avg_payment_delay'] = data[['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']].mean(axis=1)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "features = data.drop(columns=['id', 'default_payment_next_month', 'default'])\n",
        "target = data['default']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "logistic_preds = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Random Forest Classifier with Hyperparameter Tuning\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf,\n",
        "                              cv=3, n_jobs=-1, scoring='accuracy', verbose=2)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "rf_best_model = grid_search_rf.best_estimator_\n",
        "rf_best_preds = rf_best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# XGBoost Classifier with Hyperparameter Tuning\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb,\n",
        "                               cv=3, n_jobs=-1, scoring='accuracy', verbose=2)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "xgb_best_model = grid_search_xgb.best_estimator_\n",
        "xgb_best_preds = xgb_best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Improved Neural Network (Deep Learning)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "\n",
        "nn_model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "nn_model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2, verbose=1)\n",
        "nn_preds = nn_model.predict(X_test).flatten()\n",
        "\n",
        "# Model Stacking\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('rf', rf_best_model),\n",
        "    ('xgb', xgb_best_model)\n",
        "]\n",
        "\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
        "\n",
        "stacking_model.fit(X_train, y_train)\n",
        "stacking_preds = stacking_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "logistic_binary_preds = (logistic_preds >= 0.5).astype(int)\n",
        "rf_binary_preds = (rf_best_preds >= 0.5).astype(int)\n",
        "xgb_binary_preds = (xgb_best_preds >= 0.5).astype(int)\n",
        "nn_binary_preds = (nn_preds >= 0.5).astype(int)\n",
        "stacking_binary_preds = (stacking_preds >= 0.5).astype(int)\n",
        "\n",
        "# Evaluate models: ROC-AUC and Accuracy\n",
        "logistic_roc_auc = roc_auc_score(y_test, logistic_preds)\n",
        "rf_roc_auc = roc_auc_score(y_test, rf_best_preds)\n",
        "xgb_roc_auc = roc_auc_score(y_test, xgb_best_preds)\n",
        "nn_roc_auc = roc_auc_score(y_test, nn_preds)\n",
        "stacking_roc_auc = roc_auc_score(y_test, stacking_preds)\n",
        "\n",
        "logistic_acc = accuracy_score(y_test, logistic_binary_preds)\n",
        "rf_acc = accuracy_score(y_test, rf_binary_preds)\n",
        "xgb_acc = accuracy_score(y_test, xgb_binary_preds)\n",
        "nn_acc = accuracy_score(y_test, nn_binary_preds)\n",
        "stacking_acc = accuracy_score(y_test, stacking_binary_preds)\n",
        "\n",
        "# Print out results\n",
        "print(f'Logistic Regression - ROC-AUC: {logistic_roc_auc:.4f}, Accuracy: {logistic_acc:.4f}')\n",
        "print(f'Random Forest (Tuned) - ROC-AUC: {rf_roc_auc:.4f}, Accuracy: {rf_acc:.4f}')\n",
        "print(f'XGBoost (Tuned) - ROC-AUC: {xgb_roc_auc:.4f}, Accuracy: {xgb_acc:.4f}')\n",
        "print(f'Neural Network (Improved) - ROC-AUC: {nn_roc_auc:.4f}, Accuracy: {nn_acc:.4f}')\n",
        "print(f'Stacking Model - ROC-AUC: {stacking_roc_auc:.4f}, Accuracy: {stacking_acc:.4f}')\n",
        "\n",
        "# Plotting ROC-AUC comparison\n",
        "model_names = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network', 'Stacking Model']\n",
        "roc_auc_scores = [logistic_roc_auc, rf_roc_auc, xgb_roc_auc, nn_roc_auc, stacking_roc_auc]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=model_names, y=roc_auc_scores, palette=\"viridis\")\n",
        "plt.title('ROC-AUC Comparison of Different Models')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.show()\n",
        "\n",
        "# Plotting Accuracy comparison\n",
        "acc_scores = [logistic_acc, rf_acc, xgb_acc, nn_acc, stacking_acc]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=model_names, y=acc_scores, palette=\"magma\")\n",
        "plt.title('Accuracy Comparison of Different Models')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.show()\n"
      ]
    }
  ]
}